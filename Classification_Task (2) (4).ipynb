{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt \n",
    "import seaborn as sns \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score  # << ŸÖŸáŸÖ ÿ¨ÿØÿßŸã\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ebt3pid9G0vq",
    "outputId": "57b7cff1-f2cb-4eef-e298-8fd65cc12799"
   },
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "secondary_mushroom = fetch_ucirepo(id=848)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "x = secondary_mushroom.data.features\n",
    "y = secondary_mushroom.data.targets\n",
    "\n",
    "# metadata\n",
    "print(secondary_mushroom.metadata)\n",
    "\n",
    "# variable information\n",
    "print(secondary_mushroom.variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rBGp0v_ITqQ"
   },
   "source": [
    "### Mushroom Dataset Description\n",
    "\n",
    "**Target Variable (Binary Class):**  \n",
    "- `edible = e`  \n",
    "- `poisonous = p` (also includes mushrooms of unknown edibility)  \n",
    "\n",
    "---\n",
    "\n",
    "### Features (20 variables: nominal = n, metrical = m)\n",
    "\n",
    "1. **cap-diameter (m):** float number in cm  \n",
    "2. **cap-shape (n):**  \n",
    "   - bell = b  \n",
    "   - conical = c  \n",
    "   - convex = x  \n",
    "   - flat = f  \n",
    "   - sunken = s  \n",
    "   - spherical = p  \n",
    "   - others = o  \n",
    "3. **cap-surface (n):**  \n",
    "   - fibrous = i  \n",
    "   - grooves = g  \n",
    "   - scaly = y  \n",
    "   - smooth = s  \n",
    "   - shiny = h  \n",
    "   - leathery = l  \n",
    "   - silky = k  \n",
    "   - sticky = t  \n",
    "   - wrinkled = w  \n",
    "   - fleshy = e  \n",
    "4. **cap-color (n):**  \n",
    "   - brown = n  \n",
    "   - buff = b  \n",
    "   - gray = g  \n",
    "   - green = r  \n",
    "   - pink = p  \n",
    "   - purple = u  \n",
    "   - red = e  \n",
    "   - white = w  \n",
    "   - yellow = y  \n",
    "   - blue = l  \n",
    "   - orange = o  \n",
    "   - black = k  \n",
    "5. **does-bruise-bleed (n):**  \n",
    "   - bruises-or-bleeding = t  \n",
    "   - no = f  \n",
    "6. **gill-attachment (n):**  \n",
    "   - adnate = a  \n",
    "   - adnexed = x  \n",
    "   - decurrent = d  \n",
    "   - free = e  \n",
    "   - sinuate = s  \n",
    "   - pores = p  \n",
    "   - none = f  \n",
    "   - unknown = ?  \n",
    "7. **gill-spacing (n):**  \n",
    "   - close = c  \n",
    "   - distant = d  \n",
    "   - none = f  \n",
    "8. **gill-color (n):** see `cap-color` + none = f  \n",
    "9. **stem-height (m):** float number in cm  \n",
    "10. **stem-width (m):** float number in mm  \n",
    "11. **stem-root (n):**  \n",
    "    - bulbous = b  \n",
    "    - swollen = s  \n",
    "    - club = c  \n",
    "    - cup = u  \n",
    "    - equal = e  \n",
    "    - rhizomorphs = z  \n",
    "    - rooted = r  \n",
    "12. **stem-surface (n):** see `cap-surface` + none = f  \n",
    "13. **stem-color (n):** see `cap-color` + none = f  \n",
    "14. **veil-type (n):**  \n",
    "    - partial = p  \n",
    "    - universal = u  \n",
    "15. **veil-color (n):** see `cap-color` + none = f  \n",
    "16. **has-ring (n):**  \n",
    "    - ring = t  \n",
    "    - none = f  \n",
    "17. **ring-type (n):**  \n",
    "    - cobwebby = c  \n",
    "    - evanescent = e  \n",
    "    - flaring = r  \n",
    "    - grooved = g  \n",
    "    - large = l  \n",
    "    - pendant = p  \n",
    "    - sheathing = s  \n",
    "    - zone = z  \n",
    "    - scaly = y  \n",
    "    - movable = m  \n",
    "    - none = f  \n",
    "    - unknown = ?  \n",
    "18. **spore-print-color (n):** see `cap-color`  \n",
    "19. **habitat (n):**  \n",
    "    - grasses = g  \n",
    "    - leaves = l  \n",
    "    - meadows = m  \n",
    "    - paths = p  \n",
    "    - heaths = h  \n",
    "    - urban = u  \n",
    "    - waste = w  \n",
    "    - woods = d  \n",
    "20. **season (n):**  \n",
    "    - spring = s  \n",
    "    - summer = u  \n",
    "    - autumn = a  \n",
    "    - winter = w  \n",
    "\n",
    "---\n",
    "\n",
    "### Class Labels\n",
    "- **edible = e**  \n",
    "- **poisonous = p**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "TVOGWw1EGvOw",
    "outputId": "e1fdcdd4-fcaa-45cb-ade0-eba1d8ac9863"
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "CIokSeKnHDQo",
    "outputId": "01e9f62e-c03a-462e-c914-6b0f8a45ae4b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x[\"habitat\"].unique())\n",
    "\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eh14cYNyK9sb"
   },
   "source": [
    "## Mushroom Classification Project\n",
    "\n",
    "### Problem Description\n",
    "The goal of this project is to classify mushrooms as **edible (e)** or **poisonous (p)** based on 20 descriptive features such as cap shape, cap color, stem characteristics, habitat, and season.  \n",
    "This is a **binary classification problem** where the target variable is the mushroom class (`edible` or `poisonous`).  \n",
    "\n",
    "---\n",
    "\n",
    "### Project Workflow\n",
    "\n",
    "#### 1. Exploratory Data Analysis (EDA)\n",
    "- Inspect the dataset structure (rows, columns, data types).  \n",
    "- Check for missing values and duplicates.  \n",
    "- Visualize distributions of categorical features (bar plots) and numeric features (histograms, boxplots).  \n",
    "- Explore correlations between features and the target class.  \n",
    "- Look for class imbalance in the target variable (edible vs poisonous).  \n",
    "\n",
    "#### 2. Data Cleaning & Preprocessing\n",
    "- Handle missing values or unknown entries if present.  \n",
    "- Encode categorical features (e.g., **One-Hot Encoding** or **Label Encoding**).  \n",
    "- Scale numerical features (e.g., **StandardScaler** or **MinMaxScaler**).  \n",
    "- Stratify the dataset when splitting into **training and test sets** to maintain class balance.  \n",
    "- If class imbalance is significant, apply oversampling techniques (e.g., **SMOTE**).  \n",
    "\n",
    "#### 3. Feature Selection\n",
    "To reduce dimensionality and improve generalization, apply feature selection techniques such as:  \n",
    "- **Filter Methods:** Use statistical tests (e.g., Chi-square, ANOVA F-test, mutual information) to rank features.  \n",
    "- **Wrapper Methods:** Apply Recursive Feature Elimination (**RFE**) with models like Logistic Regression or Decision Trees.  \n",
    "- **Embedded Methods:** Leverage feature importance from models (e.g., Decision Tree, Random Forest, Lasso Regression).  \n",
    "- Compare model performance before and after feature selection to evaluate impact.  \n",
    "\n",
    "#### 4. Model Training & Evaluation\n",
    "Train and evaluate the following classification models:  \n",
    "- **Logistic Regression**  \n",
    "- **Decision Tree**  \n",
    "- **Random Forest**  \n",
    "- **Support Vector Machine (SVM)**  \n",
    "- **K-Nearest Neighbors (KNN)**  \n",
    "- **NaNaive Bayes**  \n",
    "\n",
    "For each model:  \n",
    "- Train on the **training set**.  \n",
    "- Evaluate on the **test set** using:  \n",
    "  - **Classification Report** (Precision, Recall, F1-score, Accuracy)  \n",
    "  - **Confusion Matrix**  \n",
    "  - **Cross-validation (optional)** for stability check  \n",
    "\n",
    "#### 5. Hyperparameter Tuning\n",
    "To improve model performance, apply hyperparameter optimization techniques:  \n",
    "- **Grid Search:** Exhaustively search all combinations of parameters (good for small parameter spaces).  \n",
    "- **Random Search:** Randomly sample parameter combinations (more efficient for large parameter spaces).  \n",
    "- **Bayesian Optimization:** Iteratively choose hyperparameters based on past performance (efficient for complex models like Random Forest, SVM, or KNN).  \n",
    "\n",
    "Compare tuned models with baseline models to measure improvement.  \n",
    "\n",
    "#### 6. Feature Importance\n",
    "- Extract **feature importance** from Decision Tree and Random Forest models.  \n",
    "- Rank features to identify the most influential ones for classification.  \n",
    "- Use feature importance as a basis for feature selection and model simplification.  \n",
    "\n",
    "#### 7. Model Comparison\n",
    "- Compare performance across all models (baseline, tuned, and with feature selection).  \n",
    "- Use metrics such as accuracy, precision, recall, F1-score, and ROC-AUC to decide the best-performing model.\n",
    "- Create plot of ROC-AUC curve of each model with different color for comparison  \n",
    "- Summarize results in a **comparison table** or **bar plot**.  \n",
    "\n",
    "---\n",
    "\n",
    "#### 8. Save best models as pickle files\n",
    "\n",
    "---\n",
    "\n",
    "### Final Deliverables\n",
    "- Preprocessed dataset ready for classification.  \n",
    "- Performance evaluation of six classification models.  \n",
    "- Insights on class imbalance and handling techniques.  \n",
    "- Feature selection results (filter, wrapper, and embedded methods).  \n",
    "- Hyperparameter tuning results from Grid Search, Random Search, and Bayesian Optimization.  \n",
    "- Feature importance ranking for better model interpretability.  \n",
    "- Final comparison and recommendation of the best-performing model for mushroom classification.  \n",
    "- pickle files of the saved models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([x,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SRZ8AepHDy1"
   },
   "outputs": [],
   "source": [
    "missper =df.isnull().sum()/len(df) *100\n",
    "plt.figure(figsize=(10,10))\n",
    "missper.sort_values().plot(kind=\"bar\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#check all null data and remove colonm if nan values more than 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column=missper[missper > 30].index\n",
    "df.drop(columns=column,inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df[\"class\"],bins=50,kde=True)\n",
    "most_freq = df[\"ring-type\"].mode()[0]\n",
    "df[\"ring-type\"] = df[\"ring-type\"].replace(\"NAN\", most_freq)\n",
    "\n",
    "most_freq = df[\"gill-attachment\"].mode()[0]\n",
    "df[\"gill-attachment\"] = df[\"gill-attachment\"].replace(\"NAN\", most_freq)\n",
    "\n",
    "most_freq = df[\"cap-surface\"].mode()[0]\n",
    "df[\"cap-surface\"] = df[\"cap-surface\"].replace(\"NAN\", most_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataAfter=df.isnull().sum()/len(df) *100\n",
    "dataAfter.sort_values().plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(df[\"class\"],bins=50,kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"class\",y=\"cap-diameter\",data=df)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colm=[\"cap-shape\",\"gill-attachment\",\"gill-color\",\"does-bruise-or-bleed\",\"season\",\"habitat\",\"ring-type\",\"has-ring\",\"stem-color\",\"cap-color\",\"class\",\"cap-surface\"]\n",
    "for z in colm:\n",
    "    encoder=LabelEncoder()\n",
    "    df[z]= encoder.fit_transform(df[z].astype(str))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"does-bruise-or-bleed\"].unique())\n",
    "print(df[\"habitat\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,6))\n",
    "plt.boxplot(df[\"stem-height\"].dropna(), vert=True, showmeans=True)\n",
    "plt.title(\"Box Plot ‚Äî stem-height\")\n",
    "plt.ylabel(\"stem-height\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr=df.corr()\n",
    "plt.figure(figsize=(10,10))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = X_train.columns[rfe.support_]\n",
    "# print(\"Selected Features:\", selected_features.tolist())\n",
    "\n",
    "# X_train_fs = X_train[selected_features]\n",
    "# X_test_fs = X_test[selected_features]\n",
    "\n",
    "# model.fit(X_train_fs, y_train)\n",
    "# y_pred = model.predict(X_test_fs)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f\"Accuracy after Feature Selection: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_features = X_train.columns[rfecv.support_]\n",
    "# print(\"Selected Features:\", selected_features.tolist())\n",
    "# X_train_selected = X_train[selected_features]\n",
    "# X_test_selected  = X_test[selected_features]\n",
    "# grid = GridSearchCV(\n",
    "#     LogisticRegression(max_iter=2000),\n",
    "#     param_grid=param_grid,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1\n",
    "# )\n",
    "# grid.fit(X_train_selected, y_train)\n",
    "\n",
    "# print(\"best_params:\", grid.best_params_)\n",
    "# print(\"acc Cross-Validation:\", grid.best_score_)\n",
    "# print(\"test set:\", grid.score(X_test_selected, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFECV, SelectFromModel\n",
    "\n",
    "# print(\"\\nüå≥ Decision Tree Feature Selection...\")\n",
    "\n",
    "# dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# # RFECV ŸÖÿπ Decision Tree\n",
    "# rfecv_dt = RFECV(\n",
    "#     estimator=dt,\n",
    "#     step=1,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     min_features_to_select=1\n",
    "# )\n",
    "# rfecv_dt.fit(X_train, y_train)\n",
    "\n",
    "# selected_features_dt = X_train.columns[rfecv_dt.support_]\n",
    "# print(\"‚úÖ DT Selected Features:\", selected_features_dt.tolist())\n",
    "\n",
    "# X_train_dt = X_train[selected_features_dt]\n",
    "# X_test_dt = X_test[selected_features_dt]\n",
    "\n",
    "# # GridSearchCV ÿπŸÑŸâ Decision Tree\n",
    "# param_grid_dt = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [None, 5, 10, 20, 50],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# grid_dt = GridSearchCV(\n",
    "#     DecisionTreeClassifier(random_state=42),\n",
    "#     param_grid=param_grid_dt,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# grid_dt.fit(X_train_dt, y_train)\n",
    "\n",
    "# print(\"\\nüèÜ Decision Tree Results:\")\n",
    "# print(\"Best Params:\", grid_dt.best_params_)\n",
    "# print(\"CV Accuracy:\", f\"{grid_dt.best_score_:.4f}\")\n",
    "# print(\"Test Accuracy:\", f\"{grid_dt.score(X_test_dt, y_test):.4f}\")\n",
    "\n",
    "# # -----------------------------\n",
    "# # Feature Selection + Tuning for Random Forest\n",
    "# # -----------------------------\n",
    "# print(\"\\nüå≤ Random Forest Feature Selection...\")\n",
    "\n",
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# # SelectFromModel ŸÖÿπ Random Forest\n",
    "# sfm = SelectFromModel(rf, threshold=\"median\")\n",
    "# sfm.fit(X_train, y_train)\n",
    "\n",
    "# selected_features_rf = X_train.columns[sfm.get_support()]\n",
    "# print(\" RF Selected Features:\", selected_features_rf.tolist())\n",
    "\n",
    "# X_train_rf = X_train[selected_features_rf]\n",
    "# X_test_rf = X_test[selected_features_rf]\n",
    "\n",
    "# param_grid_rf = {\n",
    "#     'n_estimators': [100, 200, 500],\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [None, 10, 20, 50],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "# grid_rf = GridSearchCV(\n",
    "#     RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "#     param_grid=param_grid_rf,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# grid_rf.fit(X_train_rf, y_train)\n",
    "\n",
    "# print(\"\\n Random Forest Results:\")\n",
    "# print(\"Best Params:\", grid_rf.best_params_)\n",
    "# print(\"CV Accuracy:\", f\"{grid_rf.best_score_:.4f}\")\n",
    "# print(\"Test Accuracy:\", f\"{grid_rf.score(X_test_rf, y_test):.4f}\")\n",
    "\n",
    "# importances = grid_rf.best_estimator_.feature_importances_\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'feature': selected_features_rf,\n",
    "#     'importance': importances\n",
    "# }).sort_values('importance', ascending=False)\n",
    "\n",
    "# print(\"\\n Top 10 Features from Random Forest:\")\n",
    "# print(feature_importance.head(10))\n",
    "# #Best Accuration in this with select Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nüå≥ Decision Tree Feature Selection...\")\n",
    "\n",
    "# dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# rfecv_dt = RFECV(\n",
    "#     estimator=dt,\n",
    "#     step=1,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     min_features_to_select=1\n",
    "# )\n",
    "# rfecv_dt.fit(X_train, y_train)\n",
    "\n",
    "# selected_features_dt = X_train.columns[rfecv_dt.support_]\n",
    "# X_train_dt, X_test_dt = X_train[selected_features_dt], X_test[selected_features_dt]\n",
    "\n",
    "# param_grid_dt = {\n",
    "#     'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [None, 5, 10, 20, 50],\n",
    "#     'min_samples_split': [2, 5, 10],\n",
    "#     'min_samples_leaf': [1, 2, 4]\n",
    "# }\n",
    "\n",
    "# grid_dt = GridSearchCV(\n",
    "#     DecisionTreeClassifier(random_state=42),\n",
    "#     param_grid=param_grid_dt,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "# grid_dt.fit(X_train_dt, y_train)\n",
    "\n",
    "# print(\"\\nüèÜ Decision Tree Results:\")\n",
    "# print(\"Best Params:\", grid_dt.best_params_)\n",
    "# print(\"CV Accuracy:\", f\"{grid_dt.best_score_:.4f}\")\n",
    "# print(\"Test Accuracy:\", f\"{grid_dt.score(X_test_dt, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nü§ù KNN Feature Selection...\")\n",
    "\n",
    "# # ŸáŸÜÿß ŸáŸÜÿ≥ÿ™ÿÆÿØŸÖ SelectKBest (ANOVA F-test) ÿ®ÿØŸÑ RFECV ŸÑÿ£ŸÜŸá ÿ£ÿ≥ÿ±ÿπ\n",
    "# selector_knn = SelectKBest(score_func=f_classif, k=10)\n",
    "# X_train_knn = selector_knn.fit_transform(X_train, y_train)\n",
    "# X_test_knn = selector_knn.transform(X_test)\n",
    "# selected_features_knn = X_train.columns[selector_knn.get_support()]\n",
    "# print(\"‚úÖ KNN Selected Features:\", selected_features_knn.tolist())\n",
    "# param_grid_knn = {\n",
    "#     'n_neighbors': [3, 5, 7, 9, 11],\n",
    "#     'weights': ['uniform', 'distance'],\n",
    "#     'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "# }\n",
    "# grid_knn = GridSearchCV(\n",
    "#     KNeighborsClassifier(),\n",
    "#     param_grid=param_grid_knn,\n",
    "#     cv=5,\n",
    "#     scoring='accuracy',\n",
    "#     n_jobs=-1,\n",
    "#     verbose=1\n",
    "# )\n",
    "# grid_knn.fit(X_train_knn, y_train)\n",
    "# print(\"\\nüèÜ KNN Results:\")\n",
    "# print(\"Best Params:\", grid_knn.best_params_)\n",
    "# print(\"CV Accuracy:\", f\"{grid_knn.best_score_:.4f}\")\n",
    "# print(\"Test Accuracy:\", f\"{grid_knn.score(X_test_knn, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##print(\"\\nüìä Naive Bayes Feature Selection...\")\n",
    "\n",
    "#selector_nb = SelectKBest(score_func=f_classif, k=15)\n",
    "#X_train_nb = selector_nb.fit_transform(X_train, y_train)\n",
    "#X_test_nb = selector_nb.transform(X_test)\n",
    "#selected_features_nb = X_train.columns[selector_nb.get_support()]\n",
    "#print(\"‚úÖ NB Selected Features:\", selected_features_nb.tolist())\n",
    "\n",
    "#param_grid_nb = {\n",
    "   # '#var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "#}\n",
    "#grid_nb = GridSearchCV(\n",
    "   # GaussianNB(),\n",
    "    #param_grid=param_grid_nb,\n",
    "    #cv=5,\n",
    "    #scoring='accuracy',\n",
    "   # n_jobs=-1,\n",
    "   # verbose=1\n",
    "#)\n",
    "#grid_nb.fit(X_train_nb, y_train)\n",
    "#print(\"\\n Naive Bayes Results:\")\n",
    "#print(\"Best Params:\", grid_nb.best_params_)\n",
    "#print(\"CV Accuracy:\", f\"{grid_nb.best_score_:.4f}\")\n",
    "#print(\"Test Accuracy:\", f\"{grid_nb.score(X_test_nb, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ================================\n",
    "X = df.drop(\"class\", axis=1)\n",
    "y = df[\"class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "results = {}\n",
    "#  Logistic Regression\n",
    "# ================================\n",
    "print(\"\\n Logistic Regression...\")\n",
    "\n",
    "rfecv_lr = RFECV(\n",
    "    estimator=LogisticRegression(max_iter=2000, solver=\"lbfgs\"),\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv_lr.fit(X_train, y_train)\n",
    "\n",
    "selected_features_lr = X_train.columns[rfecv_lr.support_]\n",
    "X_train_lr = X_train[selected_features_lr]\n",
    "X_test_lr = X_test[selected_features_lr]\n",
    "\n",
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'lbfgs'],\n",
    "    'penalty': ['l2']\n",
    "}\n",
    "\n",
    "grid_lr = GridSearchCV(\n",
    "    LogisticRegression(max_iter=2000, random_state=42),\n",
    "    param_grid=param_grid_lr,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_lr.fit(X_train_lr, y_train)\n",
    "\n",
    "results[\"Logistic Regression\"] = (\n",
    "    grid_lr.score(X_test_lr, y_test),\n",
    "    selected_features_lr.tolist(),\n",
    "    grid_lr.best_estimator_\n",
    ")\n",
    "\n",
    "# 2Ô∏è Decision Tree\n",
    "# ================================\n",
    "print(\"\\n Decision Tree...\")\n",
    "\n",
    "rfecv_dt = RFECV(\n",
    "    estimator=DecisionTreeClassifier(random_state=42),\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    ")\n",
    "rfecv_dt.fit(X_train, y_train)\n",
    "\n",
    "selected_features_dt = X_train.columns[rfecv_dt.support_]\n",
    "X_train_dt = X_train[selected_features_dt]\n",
    "X_test_dt = X_test[selected_features_dt]\n",
    "\n",
    "param_grid_dt = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_dt = GridSearchCV(\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    param_grid=param_grid_dt,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_dt.fit(X_train_dt, y_train)\n",
    "\n",
    "results[\"Decision Tree\"] = (\n",
    "    grid_dt.score(X_test_dt, y_test),\n",
    "    selected_features_dt.tolist(),\n",
    "    grid_dt.best_estimator_\n",
    ")\n",
    "\n",
    "# 3Ô∏èRandom Forest\n",
    "# ================================\n",
    "print(\"\\n Random Forest...\")\n",
    "\n",
    "sfm_rf = SelectFromModel(RandomForestClassifier(random_state=42), threshold=\"median\")\n",
    "sfm_rf.fit(X_train, y_train)\n",
    "\n",
    "selected_features_rf = X_train.columns[sfm_rf.get_support()]\n",
    "X_train_rf = X_train[selected_features_rf]\n",
    "X_test_rf = X_test[selected_features_rf]\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "grid_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_rf.fit(X_train_rf, y_train)\n",
    "\n",
    "results[\"Random Forest\"] = (\n",
    "    grid_rf.score(X_test_rf, y_test),\n",
    "    selected_features_rf.tolist(),\n",
    "    grid_rf.best_estimator_\n",
    ")\n",
    "\n",
    "#  KNN\n",
    "# ================================\n",
    "print(\"\\n KNN...\")\n",
    "\n",
    "selector_knn = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train_knn = selector_knn.fit_transform(X_train, y_train)\n",
    "X_test_knn = selector_knn.transform(X_test)\n",
    "\n",
    "selected_features_knn = X_train.columns[selector_knn.get_support()]\n",
    "\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid=param_grid_knn,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_knn.fit(X_train_knn, y_train)\n",
    "\n",
    "results[\"KNN\"] = (\n",
    "    grid_knn.score(X_test_knn, y_test),\n",
    "    selected_features_knn.tolist(),\n",
    "    grid_knn.best_estimator_\n",
    ")\n",
    "\n",
    "# 5Ô∏è Naive Bayes\n",
    "# ================================\n",
    "print(\"\\n Naive Bayes...\")\n",
    "\n",
    "selector_nb = SelectKBest(score_func=f_classif, k=10)\n",
    "X_train_nb = selector_nb.fit_transform(X_train, y_train)\n",
    "X_test_nb = selector_nb.transform(X_test)\n",
    "\n",
    "selected_features_nb = X_train.columns[selector_nb.get_support()]\n",
    "\n",
    "param_grid_nb = {'var_smoothing': np.logspace(-9, 0, 10)}\n",
    "\n",
    "grid_nb = GridSearchCV(\n",
    "    GaussianNB(),\n",
    "    param_grid=param_grid_nb,\n",
    "    cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_nb.fit(X_train_nb, y_train)\n",
    "\n",
    "results[\"Naive Bayes\"] = (\n",
    "    grid_nb.score(X_test_nb, y_test),\n",
    "    selected_features_nb.tolist(),\n",
    "    grid_nb.best_estimator_\n",
    ")\n",
    "\n",
    "# ================================\n",
    "print(\"\\n==============================\")\n",
    "print(\" ŸÖŸÇÿßÿ±ŸÜÿ© ÿßŸÑŸÖŸàÿØŸäŸÑÿßÿ™\")\n",
    "print(\"==============================\")\n",
    "\n",
    "for model_name, (acc, features, est) in results.items():\n",
    "    print(f\"\\nüîπ {model_name}\")\n",
    "    print(f\" Accuracy: {acc:.4f}\")\n",
    "    print(f\" Selected Features: {features}\")\n",
    "\n",
    "    # ÿßŸÑÿ™ŸÜÿ®ÿ§\n",
    "    if model_name == \"KNN\":\n",
    "        y_pred = est.predict(X_test_knn)\n",
    "    elif model_name == \"Naive Bayes\":\n",
    "        y_pred = est.predict(X_test_nb)\n",
    "    elif model_name == \"Logistic Regression\":\n",
    "        y_pred = est.predict(X_test_lr)\n",
    "    elif model_name == \"Decision Tree\":\n",
    "        y_pred = est.predict(X_test_dt)\n",
    "    elif model_name == \"Random Forest\":\n",
    "        y_pred = est.predict(X_test_rf)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(5, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# üìå Support Vector Machine (SVM) + RandomizedSearchCV\n",
    "# ================================\n",
    "print(\"\\nüìå Support Vector Machine (SVM)...\")\n",
    "\n",
    "rfecv_svm = RFECV(\n",
    "    estimator=SVC(kernel=\"linear\"),  # linear ÿπŸÑÿ¥ÿßŸÜ RFECV\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "rfecv_svm.fit(X_train, y_train)\n",
    "\n",
    "selected_features_svm = X_train.columns[rfecv_svm.support_]\n",
    "print(\"‚úÖ SVM Selected Features:\", selected_features_svm.tolist())\n",
    "\n",
    "X_train_svm = X_train[selected_features_svm]\n",
    "X_test_svm = X_test[selected_features_svm]\n",
    "\n",
    "# RandomizedSearch\n",
    "param_dist_svm = {\n",
    "    'C': uniform(0.1, 100),            \n",
    "    'kernel': ['linear', 'rbf', 'poly'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "rand_svm = RandomizedSearchCV(\n",
    "    SVC(probability=True, random_state=42),\n",
    "    param_distributions=param_dist_svm,\n",
    "    n_iter=20,          # ÿπÿØÿØ ÿßŸÑÿ™ÿ¨ÿßÿ±ÿ® ÿßŸÑÿπÿ¥Ÿàÿßÿ¶Ÿäÿ©\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rand_svm.fit(X_train_svm, y_train)\n",
    "\n",
    "print(\"\\nüèÜ SVM Results (Randomized Search):\")\n",
    "print(\"Best Params:\", rand_svm.best_params_)\n",
    "print(\"CV Accuracy:\", f\"{rand_svm.best_score_:.4f}\")\n",
    "print(\"Test Accuracy:\", f\"{rand_svm.score(X_test_svm, y_test):.4f}\")\n",
    "\n",
    "results[\"SVM\"] = (\n",
    "    rand_svm.score(X_test_svm, y_test),\n",
    "    selected_features_svm.tolist(),\n",
    "    rand_svm.best_estimator_\n",
    ")\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred_svm = rand_svm.predict(X_test_svm)\n",
    "cm = confusion_matrix(y_test, y_pred_svm)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "plt.title(\"Confusion Matrix - SVM (RandomizedSearchCV)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
